{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Intro to machine learning - Scikit-Learn\n",
    "\n",
    "**Before this notebook, you should look at [Intro_to_pandas.ipynb](Intro_to_pandas.ipynb).**\n",
    "\n",
    "In this notebook, we'll explore the Scikit Learn package for simple machine learning tasks using geoscience data examples. After this day, students will have a good overview of how to look at large datasets and solve problems with state-of-the-art machine learning tools.\n",
    "\n",
    "- Machine learning concepts\n",
    "- What is it that you’re trying to solve? How can machine learning help?\n",
    "- What's the difference between supervised and unsupervised methods?\n",
    "- What's the difference between classification and regression?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/ML_loop.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The machine learning iterative loop\n",
    "- Data — Getting the data. How to load it and put it in an `array` and/or `DataFrame`\n",
    "- Processing — data exploration, inspection, cleaning, and feature engineering.\n",
    "- Model – What is a model? Training a Scikit-Learn model.\n",
    "- Results – assessing quality and performance metrics (accuracy, recall, F1, confusion matrices)\n",
    "- Repeat – What can we do to improve performance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"../images/machine_learning_primer.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_csv('../data/training_DataFrame_processed.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Scikit-learn classifiers\n",
    "\n",
    "Let's create a model that classifies between those three classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set up the task"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make X and y\n",
    "X = df[['GR','RHOB','PE','ILD_log10']].values\n",
    "y = df['s_Facies'].values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some methods expect the data to be normalized. It's likely a good idea to normalize it no matter which method you try.\n",
    "\n",
    "`scikit-learn` has lots of scalers. The `StandardScaler` removes the mean and scales the data to unit variance.\n",
    "\n",
    "Let's take a quick look at the data before scaling:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0], X[:,1], c=df['Facies'].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:,0], X[:,1], c=df['Facies'].values)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Split the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We must split the data into a training set, a validation set, and a test set. **This is a key step in the process.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_, y_train, y_ = train_test_split(X, y, test_size=0.4, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will also split this second set (with the underscores) into two parts: one to **validate** against while training the model and selecting hyterparameters (sometimes also called the **dev** set), and one to assess the likely real-world performance of the trained model.\n",
    "\n",
    "Note that you should only predict on the **test** set once, at the end of model selection and tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, X_test, y_val, y_test = train_test_split(X_, y_, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do we feel about this? We've drawn all our records randomly from our data.\n",
    "\n",
    "### The IID assumption\n",
    "\n",
    "Our data records are not strictly independent and identically distributed. So splitting like this is not a great idea for these data. We should split by well instead."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['Well Name'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_wells = ['CHURCHMAN BIBLE', 'CROSS H CATTLE', 'LUKE G U', 'NOLAN', 'SHANKLIN', 'Recruit F9']\n",
    "features = ['GR','RHOB','PE','ILD_log10']\n",
    "\n",
    "X_train = df.loc[df['Well Name'].isin(train_wells), features].values\n",
    "y_train = df.loc[df['Well Name'].isin(train_wells), 's_Facies'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "val_wells = ['NEWBY', 'SHRIMPLIN']\n",
    "\n",
    "X_ = df.loc[df['Well Name'].isin(val_wells), features].values\n",
    "y_ = df.loc[df['Well Name'].isin(val_wells), 's_Facies'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_.shape, y_.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_val, X_test, y_val, y_test = train_test_split(X_, y_, test_size=0.5, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A simple model: _k_-NN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A fairly common method for classifying data is the _k-nearest neighbours algorithm_. The label of the object in question is determined by the neighbouring data points in the feature space used. Its most important parameter, _k_ , called `n_neighbors`, is the number of neighbors you include to make a membership decision.\n",
    "\n",
    "First we import, then instantiate, the classifier:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "\n",
    "clf = KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `fit` (train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next block is all you need to train a classifier model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `predict`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "le = LabelEncoder()\n",
    "le.fit(list(set(y_train)))\n",
    "print(le.classes_)\n",
    "print(le.transform(le.classes_))\n",
    "\n",
    "y_val_int = le.transform(y_val) \n",
    "y_pred_int = le.transform(y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "fig, axs = plt.subplots(ncols=3, figsize=(5, 10), sharey=True)\n",
    "\n",
    "colours = ['#2E86C1','#F4D03F', '#1B4F72']\n",
    "cmap_facies = ListedColormap(colours, 'indexed')\n",
    "\n",
    "ax = axs[0]\n",
    "im = ax.imshow(y_val_int[:100].reshape(-1, 1), aspect=0.05, cmap=cmap_facies)\n",
    "ax.set_title('Actual')\n",
    "\n",
    "ax = axs[1]\n",
    "im = ax.imshow(y_pred_int[:100].reshape(-1, 1), aspect=0.05, cmap=cmap_facies)\n",
    "ax.set_title('Predicted')\n",
    "\n",
    "ax = axs[2]\n",
    "ax.axis('off')\n",
    "cbar = plt.colorbar(im, ax=ax)\n",
    "cbar.set_ticks([0, 1, 2])\n",
    "cbar.set_ticklabels(le.classes_)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### `score`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looking at the results is great, but we need to get quantitative if we want to make sure that the model we trained is _good_ and produces reasonable results. The most basic test is to look at how many good predictions we would make if we predict on our **validation** data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score = clf.score(X_val, y_val)\n",
    "print(f\"The accuracy is {score*100:.1f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is the same as explicitly calling `sklearn.metrics.accuracy_score()` on the validation labels and the prediction from the validation data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "print(accuracy_score(y_val, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The _accuracy_ is just one of the _metrics_ we can use to check the quality of the predictions. There are a large number of different metrics and depending on your data and problem you may need to find the one that adjusts better to your needs.\n",
    "\n",
    "In general, _accuracy_ can be misleading, especially in datasets with unbalanced classes. A more robust metric is the `F1` metric. It combines the `precision` score and `recall`:\n",
    "\n",
    "$$ \\mathrm{F1} = \\frac{2}{\\frac{1}{\\mathrm{precision}}+ \\frac{1}{\\mathrm{recall}}} $$\n",
    "\n",
    "Scikit-learn gives a nice summary of these three metrics using `classification_report`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_val, clf.predict(X_val), digits=3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image('../images/classification_metric_definitions.png', width=400) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h3>Exercise</h3>\n",
    "\n",
    "- what is the **accuracy** of this classifier?\n",
    "- what is the **precision** for dolomite and sand? \n",
    "- what is the **recall** for dolomite and sand? \n",
    "- what is the **F1 score** for dolomite and sand? \n",
    "- Discuss with a partner the situations where accuracy or F1 score can be misleading.\n",
    "\n",
    "Note that there are 31 dolomite points and 55 sandstone points in the validation data.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('../images/2d_2class_classifier.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image('../images/dolomite_and_sandstone_worksheet.png') "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image('../images/clf_report.png') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Improving the performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on you requirements, this results might be good enough to deploy this model and use it on a \"Machine Learning Pipeline\" product but it is often not the best model you can get. Each method has a set of parameters (also known as _hyperparameters_) that can be tweaked to tune the training.\n",
    "\n",
    "For the `KNeighborsClassifier` there are a few of these parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "KNeighborsClassifier()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For this particular method, the most important parameter to adjust is `n_neighbors` (it's the `k` in the `KNeighborsClassifier`!). Unfortunately, there's no rule that tells you what's the optimal value of `k`. To overcome this we can train many models with different values of `k` and compare the results of classifications applied to the _Validation_ data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = np.arange(1, 60, 2) # Generated array of values of k to try"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Loop over each value in `nns` and store the `F1 Score`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "\n",
    "vals, trns = [], []\n",
    "pvals, ptrns = [], []\n",
    "\n",
    "for ki in k:\n",
    "    clf = KNeighborsClassifier(n_neighbors=ki)\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    y_pred = clf.predict(X_val)\n",
    "    vals.append(f1_score(y_val, y_pred, average='weighted'))\n",
    "    y_ptrn = clf.predict(X_train)\n",
    "    trns.append(f1_score(y_train, y_ptrn, average='weighted'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What value of `k` gives us the best result?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(k, vals, lw=3.0, label=\"Validation F1 score\")\n",
    "plt.plot(k, trns, lw=3.0, label=\"Training F1 score\")\n",
    "_ = plt.xlabel('n_neighbors')\n",
    "_ = plt.ylabel('clf.score')\n",
    "_ = plt.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "exercise"
    ]
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h3>Exercise</h3>\n",
    "\n",
    "- Create a new `KNeighborsClassifier` classifier where you specify the optimal number of neighbours\n",
    "- Write a new classification report for the new classifier\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using the classifier to make predictions\n",
    "\n",
    "Say you've collected / acquired some new data. How would you use your classifier on it to make predictions? \n",
    "\n",
    "- Load it\n",
    "\n",
    "  `new_data = pd.read_csv('..some_new_data_you_have_collected.csv')`\n",
    "  \n",
    "\n",
    "- Extract the relavant features and cast it as a 2-d array\n",
    "\n",
    "  `new_X = new_data[['GR','RHOB','PE','ILD_log10']].values`\n",
    "\n",
    "\n",
    "- Apply the same tranformation equations to each feature\n",
    "\n",
    "  `new_X = scalar.fit_transform(new_X)`\n",
    "\n",
    "\n",
    "- Pass it into the classifier's predict method\n",
    "\n",
    "  `y_pred = clf.predict(new_X)`\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## More methods to train models!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's pick 3 different classifiers to train different models and then compare how well they perform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neural_network import MLPClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers = {\n",
    "    \"Linear SVM\": SVC(),\n",
    "    \"Random forest\": RandomForestClassifier(),\n",
    "    \"Neural network\": MLPClassifier(),\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "classifiers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's iterate over these classifiers and print common metrics to evaluate the performance of each model using the testing dataset we defined before"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# iterate over classifiers\n",
    "for name, clf in classifiers.items():\n",
    "    clf.fit(X_train, y_train)\n",
    "    score = clf.score(X_val, y_val)\n",
    "    print(\"{:12} {}\".format(name,\"-\"*15))\n",
    "    print(classification_report(y_val, clf.predict(X_val), digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": [
     "exe"
    ]
   },
   "source": [
    "<div class=\"alert alert-success\">\n",
    "<h3>Exercise</h3>\n",
    "\n",
    "- Try other methods available in the scikit-learn library. See the list [here](http://scikit-learn.org/stable/supervised_learning.html)\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Choosing the right estimator\n",
    "\n",
    "Often the hardest part of solving a machine learning problem can be finding the right estimator for the job.\n",
    "\n",
    "Different estimators are better suited for different types of data and different problems.\n",
    "\n",
    "#### For a classifier comparison check the source code [here](http://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html)\n",
    "\n",
    "<img src=\"../images/ML_classifier_comparison_sklearn.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's try visualizing the decision boundary for our problem."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter selection"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Many of the models can be improved (or worsened) by changing the parameters that internally make the method work. It's always a good idea to check the documentation of each model (e.g. `RandomForestClassifier` [docs](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.RandomForestClassifier.html)). This process is usually called _hyperparameter tuning_."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scikit-learn offers a simple way to test different parameters for each model through a function called `GridSearchCV`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The grid search does cross-validation, instead of a validation set. So let's add our training and validation data together, so we get to use all of it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_all = np.vstack([X_train, X_val])\n",
    "y_all = np.hstack([y_train, y_val])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "parameters = {'min_samples_leaf':np.arange(1, 26, 1),\n",
    "              'max_depth':np.arange(1, 16)}\n",
    "\n",
    "rfc = RandomForestClassifier()\n",
    "clf = GridSearchCV(rfc, parameters, iid=False, cv=6, n_jobs=4, verbose=1)\n",
    "clf.fit(X_all, y_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How does the parameter space look like with respect to the score of the classifier?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = clf.cv_results_['mean_test_score']\n",
    "max_depths = clf.cv_results_[\"param_max_depth\"].data.astype(int)\n",
    "min_samples_leaf = clf.cv_results_[\"param_min_samples_leaf\"].data.astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "im = ax.imshow(scores.reshape((15, 25)),\n",
    "               origin='lower',\n",
    "               extent=[0.5, 25.5, 0.5, 15.5],\n",
    "               interpolation='none'\n",
    "              )\n",
    "ax.set_ylabel('max_depth')\n",
    "ax.set_xlabel('min_samples_leaf')\n",
    "cb = plt.colorbar(im, shrink=0.75)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`clf` can now tell us the best parameters to use with our `RandomForestClassifier`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.best_estimator_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The nice thing about `scikit-learn`'s methods is that they're all consistent and behave in the same way. Notice how`GridSearchCV` was `.fit()`. That means that we can use it to `.predict()` and it will automatically use the best set of parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X_val)\n",
    "print(classification_report(y_val, y_pred, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now might be a good time to check our **test** set — the one we reserved at the start. How might our model do on this dataset?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_check = clf.predict(X_test)\n",
    "print(classification_report(y_test, y_check, digits=3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We get the same performance as on the validation set! This is a good result, and gives us comfort that our model is going to do a reasonable job.\n",
    "\n",
    "## Confusion matrix\n",
    "\n",
    "It's also helpful to summarize the prediction tests using a [Confusion Matrix](https://en.wikipedia.org/wiki/Confusion_matrix). Scikit-learn has a function for that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "confusion_matrix(y_val, y_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But as you can see, it's not very clear... What does each row/column represent? We can help a bit:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "Counter(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected = np.unique(y_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# itertoools is a standard library for all kinds of handy iterator manipulation\n",
    "import itertools\n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(y_val, y_pred)\n",
    "\n",
    "title = 'Confusion matrix'\n",
    "cmap = plt.cm.Reds\n",
    "\n",
    "# Plot non-normalized confusion matrix.\n",
    "plt.imshow(cnf_matrix, interpolation='nearest', cmap=cmap)\n",
    "plt.title(title)\n",
    "plt.colorbar()\n",
    "tick_marks = np.arange(len(selected))\n",
    "plt.xticks(tick_marks, selected, rotation=45)\n",
    "plt.yticks(tick_marks, selected)\n",
    "\n",
    "# Print the support numbers inside the plot.\n",
    "thresh = cnf_matrix.max() / 2.\n",
    "for i, j in itertools.product(range(cnf_matrix.shape[0]), range(cnf_matrix.shape[1])):\n",
    "    plt.text(j, i, format(cnf_matrix[i, j], 'd'),\n",
    "             horizontalalignment=\"center\",\n",
    "             color=\"white\" if cnf_matrix[i, j] > thresh else \"black\")\n",
    "\n",
    "plt.ylabel('True label')\n",
    "plt.xlabel('Predicted label')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model persistence\n",
    "\n",
    "Often, we'd like to save the trained model, to go and apply it in some other application, or to share with someone else. The easiest way to save most models is as a Python 'pickle' object:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.externals import joblib\n",
    "joblib.dump(clf, 'facies_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "How do you load a saved model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = joblib.load('facies_model.pkl')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Where to go next?\n",
    "\n",
    "- More data!\n",
    "- [XGBoost](https://xgboost.readthedocs.io/en/latest/)\n",
    "- [LightGBM](https://github.com/Microsoft/LightGBM)\n",
    "- If you want to get started on Neural Networks, [Keras](https://keras.io/) provides a scikit-learn type of experience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Paper with classifier comparison ([link](https://arxiv.org/abs/1708.05070))\n",
    "\n",
    "<img src=\"../data/model_performance.jpg\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Data Science Hierarchy of Needs ([article](https://hackernoon.com/the-ai-hierarchy-of-needs-18f111fcc007))\n",
    "\n",
    "<img src=\"../images/the_ai_hierarchy_of_needs.png\"></img>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nuance\n",
    "\n",
    "- Data normalization doesn't magically fix problems with data scaling. If you're lumping a bunch of well data together and the GR, say, has different ranges in each well, then the scaled data will also have this problem. So you still need to calibrate or normalize your data to ensure it's internally consistent. This is your reponsibility; `scikit-learn` scalers don't do this for you.\n",
    "\n",
    "- Class imbalance is also your problem. You need to make sure you have good representation from all classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<hr />\n",
    "\n",
    "<p style=\"color:gray\">©2017 Agile Geoscience. Licensed CC-BY.</p>"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
